{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p style=\"text-align: justify;\">PNSC is abbreviated as Pakistan National Shipping Corporation prominent player in global shipping by maintaining diversified marine assets, providing reliable &amp; efficient shipping services to overseas and Pakistanâ€™s sea borne trade and much more are the prime responsibilities of this corporation. Its a Govt as well as <a href=\"https://jobsalert.pk/org/federal-government-jobs\" rel=\"noopener\" target=\"_blank\">Federal Government Jobs in Pakistan</a> which offered number of the vacancies from Masters to Matric level.</p>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "url = \"https://jobsalert.pk/pakistan-national-shipping-corporation-jobs-online-form/63573\"\n",
    "response = requests.get(url)\n",
    "htmlcontent = response.content\n",
    "soup = BeautifulSoup(htmlcontent, 'html.parser')\n",
    "\n",
    "\n",
    "single_psot_list = []\n",
    "\n",
    "#Post Company Logo\n",
    "sin_pos_logo = soup.select('.container-fluid .freelance-image img')[0]\n",
    "#print(\"Post Company Logo: \"+sin_pos_logo.get('data-lazy-src'))\n",
    "single_psot_list.append(sin_pos_logo.get('data-lazy-src'))\n",
    "\n",
    "#Post Main Title\n",
    "sin_pos_main_title = soup.select('.header-details h1')[0]\n",
    "#print(\"Post Main Title: \"+sin_pos_main_title.string)\n",
    "single_psot_list.append(sin_pos_main_title.string)\n",
    "\n",
    "# Post Location\n",
    "sin_pos_main_location = soup.select('.header-details p')[1]\n",
    "#print(\"Job Offer Company: \"+sin_pos_main_location.get_text())\n",
    "single_psot_list.append(sin_pos_main_location.get_text())\n",
    "\n",
    "# Post Vacancies\n",
    "sin_pos_main_vacancies = soup.select('.header-details ul li')[0]\n",
    "#print(\"Post Vacancies Count: \"+sin_pos_main_vacancies.get_text())\n",
    "single_psot_list.append(sin_pos_main_vacancies.get_text())\n",
    "\n",
    "# Right Side Details\n",
    "# Post Availability\n",
    "sin_pos_main_availibility = soup.select('.right-side-detail ul li')[0]\n",
    "#print(\"Availability: \"+sin_pos_main_availibility.get_text())\n",
    "single_psot_list.append(sin_pos_main_availibility.get_text())\n",
    "\n",
    "\n",
    "# Post Expererience\n",
    "sin_pos_main_experience = soup.select('.right-side-detail ul li')[1]\n",
    "#print(\"Post Experience: \"+sin_pos_main_experience.get_text())\n",
    "single_psot_list.append(sin_pos_main_experience.get_text())\n",
    "\n",
    "\n",
    "# Post Region\n",
    "sin_pos_main_region = soup.select('.right-side-detail ul li')[2]\n",
    "#print(\"Post Region: \"+sin_pos_main_region.get_text())\n",
    "single_psot_list.append(sin_pos_main_region.get_text())\n",
    "\n",
    "# Post Education\n",
    "sin_pos_main_education = soup.select('.right-side-detail ul li')[3]\n",
    "#print(\"Post Education: \"+sin_pos_main_education.get_text())\n",
    "single_psot_list.append(sin_pos_main_education.get_text())\n",
    "\n",
    "\n",
    "# Post Title\n",
    "sin_pos_title = soup.select('.stickyrow .stickycol9 h2')[0]\n",
    "#print(\"Post Title: \"+sin_pos_title.string)\n",
    "single_psot_list.append(sin_pos_title.string)\n",
    "\n",
    "\n",
    "# Category\n",
    "sin_pos_cat = soup.select('.stickyrow .post-meta div:nth-child(2) span a')[0]\n",
    "#print(\"Categories: \"+sin_pos_cat.string)\n",
    "single_psot_list.append(sin_pos_cat.string)\n",
    "\n",
    "\n",
    "# Offered Salary\n",
    "sin_pos_salary = soup.select('.stickyrow .ms-4 .key-field')[0]\n",
    "#print(\"Post Estimated Salary: \"+sin_pos_salary.string)\n",
    "single_psot_list.append(sin_pos_salary.string)\n",
    "\n",
    "\n",
    "sin_pos_age = soup.select('.stickyrow .ms-4 .key-field')[1]\n",
    "#print(\"Post Required Age: \"+sin_pos_age.string)\n",
    "single_psot_list.append(sin_pos_age.string)\n",
    "\n",
    "\n",
    "#sin_pos_gender = soup.select('.stickyrow .ms-4 .key-field')[2]\n",
    "#print(sin_pos_gender.string)\n",
    "\n",
    "sin_pos_designation = soup.select('.stickyrow .ms-4 .key-field')[4]\n",
    "#print(\"Post Designation: \"+sin_pos_designation.get_text())\n",
    "single_psot_list.append(sin_pos_designation.get_text())\n",
    "\n",
    "\n",
    "#Last Date\n",
    "#sin_pos_last_date = soup.select('.lastdate span')[1]\n",
    "#print(\"Post Late Date: \"+sin_pos_last_date.string)\n",
    "\n",
    "# Table of Content Start Here\n",
    "#for sin_pos_para in soup.select('.pcontent p')[1:]:\n",
    "#    print(sin_pos_para.get_text())\n",
    "\n",
    "\n",
    "if \"Table of Conte\" in str(soup.select('.pcontent')):\n",
    "    #print(soup.select('.pcontent ul')[1].get_text())\n",
    "    single_psot_list.append(soup.select('.pcontent ul')[1].get_text())\n",
    "else:\n",
    "    #print(soup.select('.pcontent ul')[0].get_text())\n",
    "    single_psot_list.append(soup.select('.pcontent ul')[0].get_text())\n",
    "\n",
    "\n",
    "#Iterate on all elements and identify them first\n",
    "# v_postions = soup.select('.pcontent *')\n",
    "# for index, vaccant_position in enumerate(v_postions):\n",
    "#     if \"Vacant\" in str(vaccant_position):   \n",
    "#         print(soup.select('.pcontent p')[index+1])\n",
    "        #print(vaccant_position)\n",
    "        #print(index)\n",
    "\n",
    "\n",
    "#How to Apply\n",
    "sin_pos_how_to_apply = soup.select('.pcontent ol')[0]\n",
    "#print(sin_pos_how_to_apply.get_text())\n",
    "single_psot_list.append(sin_pos_how_to_apply)\n",
    "\n",
    "\n",
    "for sin_para in soup.select('.pcontent h5 ~ p'):\n",
    "    #print(sin_para)\n",
    "    if \"Last Date\" in str(sin_para):\n",
    "        only_last_date = sin_para.get_text()\n",
    "        single_psot_list.append(only_last_date)\n",
    "    elif \"To Apply Online\" in str(sin_para):\n",
    "        only_apply_link = sin_para.select('a')[0].get('href')\n",
    "        single_psot_list.append(only_apply_link)\n",
    "    elif \"Address\" in str(sin_para):\n",
    "        only_address = sin_para.get_text()\n",
    "        single_psot_list.append(only_address)\n",
    "    elif \"Advertisement Image\" in str(sin_para):\n",
    "        only_link_ad_img = sin_para.select('a')[0].get('href')\n",
    "        single_psot_list.append(only_link_ad_img)\n",
    "    elif \"Application Form Will\" in str(sin_para):\n",
    "        print(\"Application Form will be available soon\")\n",
    "    elif \"Application Form\" in str(sin_para):\n",
    "        only_app_form_link = sin_para.select('a')[0].get('href')\n",
    "        single_psot_list.append(only_app_form_link)\n",
    "    else:\n",
    "        print(sin_para)\n",
    "\n",
    "inner_response = requests.get(only_link_ad_img)\n",
    "innerhtmlcontent = inner_response.content\n",
    "innersoup = BeautifulSoup(innerhtmlcontent, 'html.parser')\n",
    "\n",
    "\n",
    "inner_ad_img = innersoup.select('.pcontent img')\n",
    "#print(inner_ad_img[0].get('data-lazy-src'))\n",
    "single_psot_list.append(inner_ad_img[0].get('data-lazy-src'))\n",
    "\n",
    "single_post_list_wrapper = [single_psot_list]\n",
    "\n",
    "#print(single_post_list_wrapper)\n",
    "\n",
    "#Create DataFrame from list\n",
    "single_post_entry_df = pd.DataFrame(single_post_list_wrapper)\n",
    "single_post_entry_df.to_excel(\"new.xlsx\")\n",
    "\n",
    "#All P tag right after h5\n",
    "# print(len(soup.select('.pcontent h5 ~ p')))\n",
    "\n",
    "#Extract Only Text direct from element\n",
    "# only_last_date = sin_para.find(string=True, recursive=False)\n",
    "\n",
    "\n",
    "# sin_pos_lower_paras = soup.select('.pcontent p')[-4:]\n",
    "# print(sin_pos_lower_paras)\n",
    "# print(len(sin_pos_lower_paras))\n",
    "\n",
    "\n",
    "#Lower Last Date\n",
    "# sin_pos_lower_last_date = soup.select('div.pcontent > p:nth-child(13) > strong')[0]\n",
    "# if \"Last Date To Apply\" in str(sin_pos_lower_last_date):\n",
    "#     print(sin_pos_lower_last_date.find(string=True, recursive=False))\n",
    "# else:\n",
    "#     print(\"Lower Last Date is not found...!\")\n",
    "\n",
    "\n",
    "\n",
    "# Short Method for Extracting Table Data\n",
    "# for sin_tr in soup.select('table.table-hover-jobs tr'):\n",
    "#     for sin_td in sin_tr:\n",
    "#         print(sin_td, end='')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Main Loop of Automate Whole Process\n",
    "# for sin_tr in soup.select('table.table-hover-jobs tr'):\n",
    "#     if sin_tr.find('th'):\n",
    "#         continue\n",
    "#     # Extracting posting date\n",
    "#     job_date = sin_tr.select('td:nth-child(1)')\n",
    "#     print(job_date[0].string)\n",
    "    \n",
    "#     # Extracting post title\n",
    "#     job_title = sin_tr.select('td:nth-child(2) a')\n",
    "#     print(job_title[0].string)\n",
    "#     print(job_title[0].get('href'))\n",
    "#     # Call here function which extract a single post complete content\n",
    "\n",
    "#     # Extracting post newspaper\n",
    "#     job_newspaper = sin_tr.select('td:nth-child(3) span')\n",
    "#     print(job_newspaper[0].string)\n",
    "\n",
    "#     # Extracting post last date\n",
    "#     job_last_date = sin_tr.select('td:nth-child(4) i')\n",
    "#     print(job_last_date[0].string)\n",
    "#     print('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#bulk_jobs_posts = soup.select('table.table-hover-jobs tr')\n",
    "#print(bulk_jobs_posts)\n",
    "\n",
    "\n",
    "\n",
    "#job_tbl = soup.find_all('table', class_='job-ads-list')\n",
    "#add_headers = soup.select('.table-hover-jobs')\n",
    "#print(add_headers)\n",
    "\n",
    "#for single_header in add_headers:\n",
    "#    print(single_header)\n",
    "\n",
    "#print(link.get('src'))\n",
    "\n",
    "#print(soup.find(id='sample-id'))\n",
    "\n",
    "# for link in soup.find_all('a'):\n",
    "#         print(link.get('href'))\n",
    "\n",
    "\n",
    "\n",
    "#print(soup.prettify())\n",
    "\n",
    "#soup = BeautifulSoup(\"<p>Some<b>bad<i>HTML\")\n",
    "#print(soup.prettify())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
